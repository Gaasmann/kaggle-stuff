{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is my first try with Kaggle Notebook and competition.\n",
    "I intend to find out what I can do and start preparing some methods to tackle the data coming in my way :-)\n",
    "Don' t expect brights ideas or polished code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Library used in this notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the data\n",
    "train_data = pd.read_csv('../input/train.csv')\n",
    "test_data = pd.read_csv('../input/test.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "5            6         0       3   \n",
       "6            7         0       1   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "5                                   Moran, Mr. James    male   NaN      0   \n",
       "6                            McCarthy, Mr. Timothy J    male  54.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "5      0            330877   8.4583   NaN        Q  \n",
       "6      0             17463  51.8625   E46        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have a test set but no cross-validation one. Let's fix that\n",
    "\n",
    "perc_cv = 0.2 #x100 % \n",
    "rows = random.sample(list(train_data.index), int(len(train_data) * perc_cv))\n",
    "cv_set = train_data.ix[rows]\n",
    "training_set = train_data.drop(rows)\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First thing, let's rework our sets to extract features\n",
    "def one_hot_conversion(data):\n",
    "    return pd.get_dummies(training_set, columns=['Pclass', 'Sex', 'Embarked'])\n",
    "# useless is pretty strong, let's say not use in the model(yet)\n",
    "def remove_useless_columns(data):\n",
    "    return data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "# Then, let's extract the labels (survived)\n",
    "def extract_labels(data):\n",
    "    return (data.drop(['Survived'], axis=1),\n",
    "            data['Survived'].to_frame())\n",
    "def prepare_data(data):\n",
    "    hot_training = one_hot_conversion(data)\n",
    "    hot_training = remove_useless_columns(hot_training)\n",
    "    # Fill Na/NaN with 0 (not ideal but ok for a first try I guess)\n",
    "    hot_training = hot_training.fillna(0)\n",
    "    return hot_training\n",
    "# training data\n",
    "final_train_set = prepare_data(training_set)\n",
    "final_train_set, final_train_labels = extract_labels(final_train_set)\n",
    "# cv data\n",
    "final_cv_set = prepare_data(cv_set)\n",
    "final_cv_set, final_cv_labels = extract_labels(final_cv_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the sets to numpy dataframe\n",
    "final_train_set = final_train_set.as_matrix()\n",
    "final_train_labels = final_train_labels.as_matrix()\n",
    "final_cv_set = final_cv_set.as_matrix()\n",
    "final_cv_labels = final_cv_labels.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here the tensorflow stuff\n",
    "# First, let's try with logistic regression\n",
    "tf.reset_default_graph()\n",
    "\n",
    "data = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32)\n",
    "\n",
    "# weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal([final_train_set.shape[1], 1]))\n",
    "bias = tf.Variable(tf.truncated_normal([1,1]))\n",
    "\n",
    "# Results\n",
    "z = tf.matmul(data, weights) + bias\n",
    "H = tf.sigmoid(z)\n",
    "\n",
    "cost_function = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=z, labels=labels))\n",
    "\n",
    "# accuracy\n",
    "decision_vector = tf.round(H)\n",
    "correct_prediction = tf.equal(labels, decision_vector)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) * 100\n",
    "\n",
    "# GD\n",
    "train_step = tf.train.GradientDescentOptimizer(0.003).minimize(cost_function)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "cycle: 0\n",
      "cost training: 11.802703857421875\n",
      "accuracy training: 37.727909088134766%\n",
      "cost cv: 11.295707702636719\n",
      "accuracy cv: 37.8681640625%\n",
      "----------\n",
      "cycle: 1000\n",
      "cost training: 0.8468309640884399\n",
      "accuracy training: 54.41794967651367%\n",
      "cost cv: 0.8465902209281921\n",
      "accuracy cv: 54.41794967651367%\n",
      "----------\n",
      "cycle: 2000\n",
      "cost training: 0.6787082552909851\n",
      "accuracy training: 64.2356185913086%\n",
      "cost cv: 0.6785924434661865\n",
      "accuracy cv: 64.2356185913086%\n",
      "----------\n",
      "cycle: 3000\n",
      "cost training: 0.5912427306175232\n",
      "accuracy training: 71.10799407958984%\n",
      "cost cv: 0.5911772847175598\n",
      "accuracy cv: 71.10799407958984%\n",
      "----------\n",
      "cycle: 4000\n",
      "cost training: 0.5397866368293762\n",
      "accuracy training: 73.21178436279297%\n",
      "cost cv: 0.5397465825080872\n",
      "accuracy cv: 73.21178436279297%\n",
      "----------\n",
      "cycle: 5000\n",
      "cost training: 0.5076714754104614\n",
      "accuracy training: 78.54137420654297%\n",
      "cost cv: 0.5076459646224976\n",
      "accuracy cv: 78.54137420654297%\n",
      "----------\n",
      "cycle: 6000\n",
      "cost training: 0.4869481921195984\n",
      "accuracy training: 79.80364227294922%\n",
      "cost cv: 0.48693162202835083\n",
      "accuracy cv: 79.80364227294922%\n",
      "----------\n",
      "cycle: 7000\n",
      "cost training: 0.47322115302085876\n",
      "accuracy training: 79.80364227294922%\n",
      "cost cv: 0.4732099771499634\n",
      "accuracy cv: 79.80364227294922%\n",
      "----------\n",
      "cycle: 8000\n",
      "cost training: 0.4639032781124115\n",
      "accuracy training: 80.36465454101562%\n",
      "cost cv: 0.4638954699039459\n",
      "accuracy cv: 80.36465454101562%\n",
      "----------\n",
      "cycle: 9000\n",
      "cost training: 0.45742806792259216\n",
      "accuracy training: 80.36465454101562%\n",
      "cost cv: 0.45742255449295044\n",
      "accuracy cv: 80.36465454101562%\n",
      "----------\n",
      "cycle: 10000\n",
      "cost training: 0.4528266191482544\n",
      "accuracy training: 80.6451644897461%\n",
      "cost cv: 0.45282283425331116\n",
      "accuracy cv: 80.6451644897461%\n",
      "----------\n",
      "cycle: 11000\n",
      "cost training: 0.4494872987270355\n",
      "accuracy training: 80.6451644897461%\n",
      "cost cv: 0.4494844973087311\n",
      "accuracy cv: 80.6451644897461%\n",
      "----------\n",
      "cycle: 12000\n",
      "cost training: 0.44701510667800903\n",
      "accuracy training: 80.92566680908203%\n",
      "cost cv: 0.44701293110847473\n",
      "accuracy cv: 80.92566680908203%\n",
      "----------\n",
      "cycle: 13000\n",
      "cost training: 0.44515037536621094\n",
      "accuracy training: 80.92566680908203%\n",
      "cost cv: 0.4451487064361572\n",
      "accuracy cv: 80.92566680908203%\n",
      "----------\n",
      "cycle: 14000\n",
      "cost training: 0.44371894001960754\n",
      "accuracy training: 81.20616912841797%\n",
      "cost cv: 0.44371771812438965\n",
      "accuracy cv: 81.20616912841797%\n",
      "----------\n",
      "cycle: 15000\n",
      "cost training: 0.44260191917419434\n",
      "accuracy training: 81.6269302368164%\n",
      "cost cv: 0.44260096549987793\n",
      "accuracy cv: 81.6269302368164%\n",
      "----------\n",
      "cycle: 16000\n",
      "cost training: 0.44171664118766785\n",
      "accuracy training: 81.48667907714844%\n",
      "cost cv: 0.4417157471179962\n",
      "accuracy cv: 81.48667907714844%\n",
      "----------\n",
      "cycle: 17000\n",
      "cost training: 0.4410051703453064\n",
      "accuracy training: 81.20616912841797%\n",
      "cost cv: 0.4410044848918915\n",
      "accuracy cv: 81.20616912841797%\n",
      "----------\n",
      "cycle: 18000\n",
      "cost training: 0.4404255747795105\n",
      "accuracy training: 81.20616912841797%\n",
      "cost cv: 0.44042524695396423\n",
      "accuracy cv: 81.20616912841797%\n",
      "----------\n",
      "cycle: 19000\n",
      "cost training: 0.43994829058647156\n",
      "accuracy training: 81.06591796875%\n",
      "cost cv: 0.4399479925632477\n",
      "accuracy cv: 81.06591796875%\n",
      "----------\n",
      "cycle: 20000\n",
      "cost training: 0.4395509660243988\n",
      "accuracy training: 81.06591796875%\n",
      "cost cv: 0.43955057859420776\n",
      "accuracy cv: 81.06591796875%\n",
      "----------\n",
      "cycle: 21000\n",
      "cost training: 0.4392170310020447\n",
      "accuracy training: 81.20616912841797%\n",
      "cost cv: 0.43921682238578796\n",
      "accuracy cv: 81.20616912841797%\n",
      "----------\n",
      "cycle: 22000\n",
      "cost training: 0.4389341175556183\n",
      "accuracy training: 81.06591796875%\n",
      "cost cv: 0.4389339089393616\n",
      "accuracy cv: 81.06591796875%\n",
      "----------\n",
      "cycle: 23000\n",
      "cost training: 0.43869274854660034\n",
      "accuracy training: 80.78540802001953%\n",
      "cost cv: 0.438692569732666\n",
      "accuracy cv: 80.78540802001953%\n",
      "----------\n",
      "cycle: 24000\n",
      "cost training: 0.4384855628013611\n",
      "accuracy training: 80.92566680908203%\n",
      "cost cv: 0.43848541378974915\n",
      "accuracy cv: 80.92566680908203%\n",
      "----------\n",
      "cycle: 25000\n",
      "cost training: 0.43830668926239014\n",
      "accuracy training: 80.92566680908203%\n",
      "cost cv: 0.4383064806461334\n",
      "accuracy cv: 80.92566680908203%\n",
      "----------\n",
      "cycle: 26000\n",
      "cost training: 0.4381515383720398\n",
      "accuracy training: 80.6451644897461%\n",
      "cost cv: 0.4381515383720398\n",
      "accuracy cv: 80.6451644897461%\n",
      "----------\n",
      "cycle: 27000\n",
      "cost training: 0.4380166232585907\n",
      "accuracy training: 80.6451644897461%\n",
      "cost cv: 0.43801644444465637\n",
      "accuracy cv: 80.6451644897461%\n",
      "----------\n",
      "cycle: 28000\n",
      "cost training: 0.4378986656665802\n",
      "accuracy training: 80.6451644897461%\n",
      "cost cv: 0.43789851665496826\n",
      "accuracy cv: 80.6451644897461%\n",
      "----------\n",
      "cycle: 29000\n",
      "cost training: 0.4377954602241516\n",
      "accuracy training: 80.78540802001953%\n",
      "cost cv: 0.4377952814102173\n",
      "accuracy cv: 80.78540802001953%\n",
      "----------\n",
      "cycle: 30000\n",
      "cost training: 0.4377046227455139\n",
      "accuracy training: 80.6451644897461%\n",
      "cost cv: 0.43770450353622437\n",
      "accuracy cv: 80.6451644897461%\n",
      "----------\n",
      "cycle: 31000\n",
      "cost training: 0.4376246929168701\n",
      "accuracy training: 80.6451644897461%\n",
      "cost cv: 0.43762463331222534\n",
      "accuracy cv: 80.6451644897461%\n",
      "----------\n",
      "cycle: 32000\n",
      "cost training: 0.43755432963371277\n",
      "accuracy training: 80.78540802001953%\n",
      "cost cv: 0.4375542402267456\n",
      "accuracy cv: 80.78540802001953%\n",
      "----------\n",
      "cycle: 33000\n",
      "cost training: 0.4374919831752777\n",
      "accuracy training: 80.5049057006836%\n",
      "cost cv: 0.4374919533729553\n",
      "accuracy cv: 80.5049057006836%\n",
      "----------\n",
      "cycle: 34000\n",
      "cost training: 0.4374368190765381\n",
      "accuracy training: 80.36465454101562%\n",
      "cost cv: 0.4374367892742157\n",
      "accuracy cv: 80.36465454101562%\n",
      "----------\n",
      "cycle: 35000\n",
      "cost training: 0.43738803267478943\n",
      "accuracy training: 80.36465454101562%\n",
      "cost cv: 0.43738794326782227\n",
      "accuracy cv: 80.36465454101562%\n",
      "----------\n",
      "cycle: 36000\n",
      "cost training: 0.4373447299003601\n",
      "accuracy training: 80.36465454101562%\n",
      "cost cv: 0.43734458088874817\n",
      "accuracy cv: 80.36465454101562%\n",
      "----------\n",
      "cycle: 37000\n",
      "cost training: 0.43730607628822327\n",
      "accuracy training: 80.36465454101562%\n",
      "cost cv: 0.43730607628822327\n",
      "accuracy cv: 80.36465454101562%\n",
      "----------\n",
      "cycle: 38000\n",
      "cost training: 0.4372718632221222\n",
      "accuracy training: 80.08415222167969%\n",
      "cost cv: 0.43727177381515503\n",
      "accuracy cv: 80.08415222167969%\n",
      "----------\n",
      "cycle: 39000\n",
      "cost training: 0.43724125623703003\n",
      "accuracy training: 80.08415222167969%\n",
      "cost cv: 0.4372413158416748\n",
      "accuracy cv: 80.08415222167969%\n",
      "----------\n",
      "cycle: 40000\n",
      "cost training: 0.4372141659259796\n",
      "accuracy training: 79.94390106201172%\n",
      "cost cv: 0.4372141361236572\n",
      "accuracy cv: 79.94390106201172%\n",
      "----------\n",
      "cycle: 41000\n",
      "cost training: 0.43718981742858887\n",
      "accuracy training: 79.94390106201172%\n",
      "cost cv: 0.43718981742858887\n",
      "accuracy cv: 79.94390106201172%\n",
      "----------\n",
      "cycle: 42000\n",
      "cost training: 0.437168151140213\n",
      "accuracy training: 79.94390106201172%\n",
      "cost cv: 0.437168151140213\n",
      "accuracy cv: 79.94390106201172%\n",
      "----------\n",
      "cycle: 43000\n",
      "cost training: 0.43714872002601624\n",
      "accuracy training: 79.94390106201172%\n",
      "cost cv: 0.43714869022369385\n",
      "accuracy cv: 79.94390106201172%\n",
      "----------\n",
      "cycle: 44000\n",
      "cost training: 0.4371313452720642\n",
      "accuracy training: 79.94390106201172%\n",
      "cost cv: 0.4371313452720642\n",
      "accuracy cv: 79.94390106201172%\n",
      "----------\n",
      "cycle: 45000\n",
      "cost training: 0.4371159076690674\n",
      "accuracy training: 80.08415222167969%\n",
      "cost cv: 0.4371158480644226\n",
      "accuracy cv: 80.08415222167969%\n",
      "----------\n",
      "cycle: 46000\n",
      "cost training: 0.43710190057754517\n",
      "accuracy training: 80.08415222167969%\n",
      "cost cv: 0.43710193037986755\n",
      "accuracy cv: 80.08415222167969%\n",
      "----------\n",
      "cycle: 47000\n",
      "cost training: 0.4370895028114319\n",
      "accuracy training: 80.08415222167969%\n",
      "cost cv: 0.4370895326137543\n",
      "accuracy cv: 80.08415222167969%\n",
      "----------\n",
      "cycle: 48000\n",
      "cost training: 0.43707817792892456\n",
      "accuracy training: 80.08415222167969%\n",
      "cost cv: 0.43707823753356934\n",
      "accuracy cv: 80.08415222167969%\n",
      "----------\n",
      "cycle: 49000\n",
      "cost training: 0.4370681345462799\n",
      "accuracy training: 80.08415222167969%\n",
      "cost cv: 0.43706807494163513\n",
      "accuracy cv: 80.08415222167969%\n",
      "----------\n",
      "cycle: 50000\n",
      "cost training: 0.43705904483795166\n",
      "accuracy training: 80.08415222167969%\n",
      "cost cv: 0.43705904483795166\n",
      "accuracy cv: 80.08415222167969%\n",
      "----------\n",
      "cycle: 51000\n",
      "cost training: 0.4370509684085846\n",
      "accuracy training: 80.08415222167969%\n",
      "cost cv: 0.4370509386062622\n",
      "accuracy cv: 80.08415222167969%\n",
      "----------\n",
      "cycle: 52000\n",
      "cost training: 0.43704351782798767\n",
      "accuracy training: 80.08415222167969%\n",
      "cost cv: 0.43704357743263245\n",
      "accuracy cv: 80.08415222167969%\n",
      "----------\n",
      "cycle: 53000\n",
      "cost training: 0.4370368421077728\n",
      "accuracy training: 80.08415222167969%\n",
      "cost cv: 0.43703675270080566\n",
      "accuracy cv: 80.08415222167969%\n",
      "----------\n",
      "cycle: 54000\n",
      "cost training: 0.43703094124794006\n",
      "accuracy training: 80.08415222167969%\n",
      "cost cv: 0.43703094124794006\n",
      "accuracy cv: 80.08415222167969%\n",
      "----------\n",
      "cycle: 55000\n",
      "cost training: 0.4370254874229431\n",
      "accuracy training: 80.08415222167969%\n",
      "cost cv: 0.43702542781829834\n",
      "accuracy cv: 80.08415222167969%\n",
      "----------\n",
      "cycle: 56000\n",
      "cost training: 0.4370206296443939\n",
      "accuracy training: 80.08415222167969%\n",
      "cost cv: 0.4370206296443939\n",
      "accuracy cv: 80.08415222167969%\n",
      "----------\n",
      "cycle: 57000\n",
      "cost training: 0.43701615929603577\n",
      "accuracy training: 80.08415222167969%\n",
      "cost cv: 0.4370161294937134\n",
      "accuracy cv: 80.08415222167969%\n",
      "----------\n",
      "cycle: 58000\n",
      "cost training: 0.4370122253894806\n",
      "accuracy training: 80.08415222167969%\n",
      "cost cv: 0.4370121955871582\n",
      "accuracy cv: 80.08415222167969%\n",
      "----------\n",
      "cycle: 59000\n",
      "cost training: 0.4370085597038269\n",
      "accuracy training: 80.08415222167969%\n",
      "cost cv: 0.4370085597038269\n",
      "accuracy cv: 80.08415222167969%\n",
      "----------\n",
      "cycle: 60000\n",
      "cost training: 0.4370052218437195\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.43700534105300903\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 61000\n",
      "cost training: 0.4370022118091583\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.4370022118091583\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 62000\n",
      "cost training: 0.4369995594024658\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.43699952960014343\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 63000\n",
      "cost training: 0.4369969964027405\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.4369969964027405\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 64000\n",
      "cost training: 0.4369948208332062\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.4369948208332062\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 65000\n",
      "cost training: 0.43699270486831665\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.43699270486831665\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 66000\n",
      "cost training: 0.436990886926651\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.436990886926651\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 67000\n",
      "cost training: 0.43698903918266296\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.43698903918266296\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 68000\n",
      "cost training: 0.43698757886886597\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.43698760867118835\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 69000\n",
      "cost training: 0.4369862377643585\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.43698620796203613\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 70000\n",
      "cost training: 0.4369848966598511\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.4369848966598511\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 71000\n",
      "cost training: 0.436983585357666\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.43698355555534363\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 72000\n",
      "cost training: 0.43698254227638245\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.43698257207870483\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 73000\n",
      "cost training: 0.4369814991950989\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.43698152899742126\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 74000\n",
      "cost training: 0.43698060512542725\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.43698060512542725\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 75000\n",
      "cost training: 0.4369797110557556\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.4369797110557556\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 76000\n",
      "cost training: 0.43697893619537354\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.43697887659072876\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 77000\n",
      "cost training: 0.43697816133499146\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.43697816133499146\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 78000\n",
      "cost training: 0.4369775056838989\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.4369775056838989\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 79000\n",
      "cost training: 0.4369767904281616\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.4369768500328064\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 80000\n",
      "cost training: 0.43697628378868103\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.43697628378868103\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 81000\n",
      "cost training: 0.4369758069515228\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.43697577714920044\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 82000\n",
      "cost training: 0.4369753301143646\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.4369753301143646\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 83000\n",
      "cost training: 0.43697473406791687\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.43697473406791687\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 84000\n",
      "cost training: 0.436974436044693\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.436974436044693\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 85000\n",
      "cost training: 0.43697401881217957\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.43697401881217957\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 86000\n",
      "cost training: 0.4369736611843109\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.4369736611843109\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 87000\n",
      "cost training: 0.4369732737541199\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.4369732737541199\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 88000\n",
      "cost training: 0.43697288632392883\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.4369729459285736\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 89000\n",
      "cost training: 0.43697258830070496\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.43697258830070496\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 90000\n",
      "cost training: 0.4369724690914154\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.4369724690914154\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 91000\n",
      "cost training: 0.43697217106819153\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.43697217106819153\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 92000\n",
      "cost training: 0.4369717836380005\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.4369718134403229\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 93000\n",
      "cost training: 0.4369715750217438\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.4369715750217438\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 94000\n",
      "cost training: 0.43697139620780945\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.43697139620780945\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 95000\n",
      "cost training: 0.4369712173938751\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.43697118759155273\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 96000\n",
      "cost training: 0.436970978975296\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.436970978975296\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 97000\n",
      "cost training: 0.43697071075439453\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.43697071075439453\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 98000\n",
      "cost training: 0.4369705021381378\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.4369705021381378\n",
      "accuracy cv: 80.22440338134766%\n",
      "----------\n",
      "cycle: 99000\n",
      "cost training: 0.43697038292884827\n",
      "accuracy training: 80.22440338134766%\n",
      "cost cv: 0.43697038292884827\n",
      "accuracy cv: 80.22440338134766%\n"
     ]
    }
   ],
   "source": [
    "# Let go training\n",
    "\n",
    "# TF Session + init\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Turbo training\n",
    "nb_cycle = 100000\n",
    "\n",
    "# cost/acc memory\n",
    "cost_training_mem = []\n",
    "cost_cv_mem = []\n",
    "acc_training_mem = []\n",
    "acc_cv_mem = []\n",
    "\n",
    "for cycle in range(nb_cycle):\n",
    "    # training\n",
    "    ts, cost, acc = sess.run([train_step, cost_function, accuracy],\n",
    "                            feed_dict={data: final_train_set, labels: final_train_labels})\n",
    "    cost_training_mem.append(cost)\n",
    "    acc_training_mem.append(acc)\n",
    "    # Cross validation\n",
    "    cost, acc = sess.run([cost_function, accuracy],\n",
    "                         feed_dict={data: final_cv_set, labels: final_cv_labels})\n",
    "    cost_cv_mem.append(cost)\n",
    "    acc_cv_mem.append(acc)\n",
    "    if cycle % 1000 == 0:\n",
    "        print('-' * 10)\n",
    "        print('cycle: {}'.format(cycle))\n",
    "        print('cost training: {}'.format(cost_training_mem[-1]))\n",
    "        print('accuracy training: {}%'.format(acc_training_mem[-1]))\n",
    "        print('cost cv: {}'.format(cost_cv_mem[-1]))\n",
    "        print('accuracy cv: {}%'.format(acc_cv_mem[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSikVIDTQlVIqQGigK6VUgPh/Or2WA0XwC5kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f961821ef28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "limit_step = 36\n",
    "plt.plot(np.array([cost_training_mem[:limit_step],cost_cv_mem[:limit_step]]).transpose())\n",
    "plt.title('cost evolution during training')\n",
    "#plt.ylim(0.70,2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "81% percent, not bad for a first try using a simple logistic regression.\n",
    "\n",
    "The cost function gives similar result for training and cross validation sets. This is a sign that the model is **underfitting** the data. This was expected. Several leads to fix that:\n",
    "\n",
    "- Find a good way to deal with NaN values. I put 0, it might not be the right thing to do. Maybe NaNs are introduced during the import of the data file. (Notes talk about estimated age for example)\n",
    "- Get more data. that would be difficult. Or maybe we can borrow a DeLorean and sell more tickets to the Titanic but that would be a bit difficult and criminal\n",
    "- Engineer new features. Kaggle provides a [link][1] about that. And some of the data are not used in this model (cabin, name, ticket number). Maybe:\n",
    " - ticket number has a relation with where you are on the ship (especially for 3rd class) or maybe it was used to select who would have a place on lifeboats;\n",
    " - the format of the \"cabin\" column indicates where your cabin is on the boat. Then we might be able to extract a geographic position on the ship;\n",
    "  - we can use good old higher order feature introduction (X1 => X1^2, ...);\n",
    "  - increase model's capacity. Like using neural network instead of our single logistic unit.\n",
    "\n",
    "\n",
    "  [1]: https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
